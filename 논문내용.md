# 모듈 기반 웹 데이터 분석 파이프라인 플랫폼: Pyodide를 활용한 브라우저 내 Python 실행 기반 시각적 분석 도구

## Abstract (초록)

본 논문에서는 웹 기반의 시각적 데이터 분석 플랫폼을 제안한다. 본 플랫폼은 복잡한 데이터 분석 작업을 모듈 단위로 구성하여 비전문가도 쉽게 접근할 수 있도록 설계되었으며, Pyodide를 활용하여 브라우저 내에서 Python 코드를 직접 실행함으로써 별도의 백엔드 서버 없이 완전한 데이터 분석 워크플로우를 제공한다. 본 시스템의 핵심 특징은 웹 애플리케이션으로 개발되었지만, 모든 데이터 분석 작업이 Python 기반으로 실행되어 실제 Python 환경에서의 검증이 가능하다는 점이다. 모듈 기반 아키텍처를 통해 데이터 전처리, 통계 분석, 머신러닝 모델링 등 다양한 분석 기능을 시각적으로 조합할 수 있으며, 각 모듈의 실행 결과를 실시간으로 확인하고 파이프라인을 Python 코드로 변환하여 독립 실행 및 검증이 가능하다.

**Keywords**: 데이터 분석, 웹 기반 플랫폼, Pyodide, 모듈 기반 아키텍처, 시각적 파이프라인, Python 실행

---

## 1. Introduction (서론)

### 1.1 연구 배경 및 동기

데이터 분석의 중요성이 증가함에 따라, 다양한 도메인의 전문가들이 데이터 분석 도구에 대한 접근성을 요구하고 있다. 그러나 기존의 데이터 분석 도구들은 대부분 프로그래밍 지식이 필요하거나, 복잡한 설정과 설치 과정을 요구하여 비전문가의 접근을 어렵게 만든다. 또한 웹 기반 도구들은 대부분 백엔드 서버를 필요로 하거나, JavaScript로 재구현된 알고리즘을 사용하여 Python 표준 라이브러리와의 결과 일치성을 보장하기 어렵다.

### 1.2 연구 목적

본 연구의 목적은 다음과 같다:

1. **접근성 향상**: 프로그래밍 지식이 없는 사용자도 시각적 인터페이스를 통해 데이터 분석을 수행할 수 있는 플랫폼 제공
2. **검증 가능성 보장**: Python 표준 라이브러리를 직접 사용하여 결과의 신뢰성 및 재현성 보장
3. **확장성 제공**: 모듈 기반 구조를 통해 새로운 분석 기능을 쉽게 추가할 수 있는 확장 가능한 아키텍처 구현
4. **서버리스 실행**: Pyodide를 활용하여 별도의 백엔드 서버 없이 브라우저에서 모든 분석 수행

### 1.3 논문 구성

본 논문은 다음과 같이 구성된다. 2장에서는 시스템의 전체 아키텍처와 모듈 시스템을 설명하고, 3장에서는 Pyodide 통합 및 기술적 구현 세부사항을 다룬다. 4장에서는 제공되는 주요 기능과 모듈들을 상세히 설명하고, 5장에서는 실제 사용 사례를 제시한다. 마지막으로 6장에서는 결론 및 향후 연구 방향을 제시한다.

---

## 2. System Architecture (시스템 구조)

### 2.1 전체 아키텍처

본 플랫폼은 3계층 아키텍처로 구성되어 있으며, 각 계층은 명확한 책임을 가진다.

#### 2.1.1 프레젠테이션 계층 (Presentation Layer)

프레젠테이션 계층은 React 기반의 사용자 인터페이스를 제공한다:

- **시각적 파이프라인 편집기**: Canvas 기반의 드래그 앤 드롭 인터페이스로 모듈을 배치하고 연결
- **모듈 속성 설정 패널**: 각 모듈의 파라미터를 동적으로 설정
- **결과 미리보기 및 상세 보기 모달**: 각 모듈의 실행 결과를 실시간으로 확인

#### 2.1.2 비즈니스 로직 계층 (Business Logic Layer)

비즈니스 로직 계층은 파이프라인 실행을 관리한다:

- **모듈 실행 관리**: 각 모듈의 실행 상태 (Pending, Running, Success, Error) 관리
- **의존성 해결**: 모듈 간 연결 관계를 분석하여 의존성 그래프 구성
- **파이프라인 실행 순서 결정**: 위상 정렬(Topological Sort) 알고리즘을 사용하여 올바른 실행 순서 결정
- **데이터 흐름 제어**: 모듈 간 데이터 전달 및 변환 관리

#### 2.1.3 실행 계층 (Execution Layer)

실행 계층은 실제 데이터 분석 작업을 수행한다:

- **Pyodide 통합**: WebAssembly를 통해 브라우저에서 Python 실행
- **Python 라이브러리 통합**: pandas, numpy, scikit-learn, statsmodels 등 표준 라이브러리 사용
- **데이터 변환**: JavaScript와 Python 간 데이터 변환 (JSON ↔ pandas DataFrame)
- **결과 반환**: Python 실행 결과를 JavaScript 객체로 변환하여 반환

### 2.2 모듈 시스템

시스템은 기능별로 모듈을 카테고리화하여 제공한다:

#### 2.2.1 Data Preprocess (데이터 전처리)

- **Load Data**: CSV 파일 로드
- **Select Data**: 열 선택 및 제거
- **Data Filtering**: 조건에 따른 데이터 필터링
- **Handle Missing Values**: 결측치 처리 (제거, 평균/중앙값 대체 등)
- **Encode Categorical**: 범주형 데이터 인코딩
- **Normalize Data**: 데이터 정규화 및 스케일링
- **Transform Data**: 수학적 변환 적용
- **Resample Data**: 클래스 불균형 처리 (SMOTE, NearMiss 등)

#### 2.2.2 Stat Lab (통계 분석)

- **Statistics**: 기술 통계 (평균, 표준편차, 분위수 등)
- **Outlier Detector**: 이상치 탐지 (IQR, Z-score, Isolation Forest, Boxplot)
- **Hypothesis Testing**: 가설 검정 (t-검정, 카이제곱, ANOVA, KS-검정, Shapiro-Wilk, Levene)
- **Correlation**: 상관분석 (Pearson, Spearman, Kendall, Cramér's V)
- **Column Plot**: 데이터 시각화

#### 2.2.3 Data Analysis (데이터 분석)

- **Split Data**: 학습/테스트 데이터 분할
- **Train Model**: 모델 학습
- **Score Model**: 예측 수행
- **Evaluate Model**: 모델 평가

#### 2.2.4 Supervised Learning (지도 학습)

- **Linear Regression**: 선형 회귀
- **Logistic Regression**: 로지스틱 회귀
- **Poisson Regression**: 포아송 회귀
- **Negative Binomial Regression**: 음이항 회귀
- **Decision Tree**: 의사결정나무
- **Random Forest**: 랜덤 포레스트
- **SVM**: 서포트 벡터 머신
- **LDA**: 선형 판별 분석
- **Naive Bayes**: 나이브 베이즈
- **KNN**: K-최근접 이웃

#### 2.2.5 Unsupervised Learning (비지도 학습)

- **K-Means**: K-평균 클러스터링
- **Hierarchical Clustering**: 계층적 클러스터링
- **DBSCAN**: 밀도 기반 클러스터링
- **PCA**: 주성분 분석

#### 2.2.6 Tradition Analysis (전통적 통계 분석)

- **Stat Models**: 통계 모델 정의 (OLS, Logit 등)
- **Result Model**: 모델 적합 및 요약
- **Predict Model**: 예측 수행

### 2.3 데이터 흐름 구조

각 모듈은 입력 포트(input ports)와 출력 포트(output ports)를 가지며, 모듈 간 연결(Connection)을 통해 데이터가 전달된다. 시스템은 다음과 같은 데이터 타입을 지원한다:

- **`DataPreview`**: 테이블 형태의 데이터 (pandas DataFrame)
- **`TrainedModelOutput`**: 학습된 모델 객체
- **`EvaluationOutput`**: 모델 평가 결과 (정확도, 정밀도, 재현율, F1-score 등)
- **`StatisticsOutput`**: 기술 통계 결과
- **`OutlierDetectorOutput`**: 이상치 탐지 결과 (열별 이상치 인덱스 및 통계)
- **`HypothesisTestingOutput`**: 가설 검정 결과 (검정 통계량, p-value, 결론)
- **`CorrelationOutput`**: 상관분석 결과 (상관계수 행렬, 시각화 이미지)

모듈 간 연결은 방향성 그래프(Directed Graph)로 표현되며, 시스템은 위상 정렬 알고리즘을 사용하여 올바른 실행 순서를 결정한다.

---

## 3. Implementation (구현)

### 3.1 Pyodide 통합

Pyodide는 WebAssembly를 통해 브라우저에서 Python을 실행할 수 있게 해주는 기술이다. 본 플랫폼은 Pyodide를 활용하여 다음과 같은 기능을 구현한다:

#### 3.1.1 서버리스 실행

별도의 백엔드 서버 없이 브라우저에서 모든 분석을 수행한다. Pyodide는 CDN을 통해 동적으로 로드되며, 필요한 Python 패키지도 런타임에 설치된다.

#### 3.1.2 표준 라이브러리 사용

Python의 표준 데이터 분석 라이브러리를 직접 사용함으로써:

- **결과의 정확성**: JavaScript로 재구현하지 않아 Python 표준 라이브러리와 동일한 결과 보장
- **검증 가능성**: 각 모듈에서 실행되는 Python 코드는 실제 Python 환경에서도 동일하게 실행 가능
- **유지보수성**: Python 라이브러리 업데이트에 따라 자동으로 최신 기능 활용 가능

#### 3.1.3 에러 처리 및 타임아웃 관리

- **타임아웃 관리**: 장시간 실행되는 작업에 대한 타임아웃 처리 (기본 60초)
- **에러 핸들링**: Python 실행 중 발생하는 에러를 JavaScript로 전달하여 사용자에게 명확한 에러 메시지 표시
- **진행 상황 표시**: 모듈 실행 상태를 실시간으로 UI에 반영

### 3.2 지원되는 Python 라이브러리 및 버전

본 플랫폼은 **Pyodide v0.24.1**을 사용하며, 다음 Python 라이브러리들을 지원한다:

| 라이브러리       | 버전 범위 | 용도                           |
| ---------------- | --------- | ------------------------------ |
| **Python**       | 3.11      | Python 런타임                  |
| **NumPy**        | 1.24.x    | 수치 연산 및 배열 처리         |
| **pandas**       | 2.0.x     | 데이터 조작 및 분석            |
| **scikit-learn** | 1.2.x     | 머신러닝 알고리즘              |
| **SciPy**        | 1.10.x    | 과학 계산 및 통계              |
| **statsmodels**  | 0.14.x    | 통계 모델링 (필요시 동적 로드) |
| **Matplotlib**   | 3.7.x     | 데이터 시각화                  |

**참고**: Pyodide는 특정 버전의 패키지들을 번들로 제공하며, Pyodide 0.24.1은 Python 3.11 기반이다. 정확한 패키지 버전은 Pyodide 릴리스에 포함된 버전을 따른다.

### 3.3 브라우저 호환성

Pyodide는 WebAssembly를 사용하므로 WebAssembly를 지원하는 최신 브라우저에서 동작한다. 본 플랫폼은 다음 브라우저에서 테스트되었으며 권장된다:

**최소 요구사항 (기본 기능):**

- **Chrome/Edge**: 57 이상 (2017년 3월)
- **Firefox**: 52 이상 (2017년 3월)
- **Safari**: 11 이상 (2017년 9월)

**권장 브라우저 (최적 성능):**

- **Chrome/Edge**: 112 이상 (2023년 3월) - **권장**
- **Firefox**: 112 이상 (2023년 4월) - **권장**
- **Safari**: 16.4 이상 (2023년 3월) - **권장**

**주요 제약사항:**

- **인터넷 연결 필요**: Pyodide 및 패키지는 CDN에서 동적으로 로드된다
- **메모리 요구사항**: 대용량 데이터셋의 경우 충분한 메모리 필요 (권장: 4GB 이상)
- **초기 로딩 시간**: Pyodide 및 패키지 로딩에 30-90초 소요 가능
- **모바일 브라우저**: 제한적으로 지원되며, 성능이 데스크톱보다 낮을 수 있음

### 3.4 모듈 확장성

새로운 분석 기능을 추가하기 위해서는 다음 단계를 따른다:

1. **`ModuleType` enum에 새 모듈 타입 추가**: TypeScript 타입 정의
2. **`constants.ts`에 모듈 정의 추가**: 모듈 이름, 아이콘, 설명 등 메타데이터
3. **`types.ts`에 출력 타입 정의**: 모듈의 출력 데이터 구조 정의
4. **`pyodideRunner.ts`에 Python 실행 함수 구현**: 실제 분석 로직을 Python으로 작성
5. **`App.tsx`에 실행 로직 추가**: 모듈 실행 및 결과 처리 로직
6. **필요시 전용 Preview Modal 컴포넌트 생성**: 결과를 시각화하는 UI 컴포넌트

이러한 구조화된 확장 방식을 통해 새로운 통계 분석 기능을 쉽게 추가할 수 있으며, 모든 모듈이 동일한 인터페이스를 따르므로 시스템의 일관성이 유지된다.

### 3.5 상태 관리

시스템은 React의 `useState`와 `useCallback`을 활용하여 상태를 관리한다:

- **모듈 상태 관리**: 각 모듈의 실행 상태 (Pending, Running, Success, Error)를 추적
- **데이터 흐름 추적**: 모듈 간 데이터 전달 및 의존성 관리
- **실행 순서 최적화**: 위상 정렬을 통한 효율적인 파이프라인 실행
- **UI 업데이트**: 모듈 상태 변경 시 자동으로 UI 업데이트

---

## 4. Features and Modules (기능 및 모듈)

### 4.1 Python 기반 분석 실행

본 플랫폼의 가장 중요한 특징은 모든 데이터 분석 작업이 Pyodide를 통해 브라우저 내에서 Python으로 실행된다는 점이다. 이를 통해:

- **검증 가능성**: 각 모듈에서 실행되는 Python 코드는 실제 Python 환경에서도 동일하게 실행 가능
- **표준 라이브러리 활용**: pandas, numpy, scikit-learn, statsmodels 등 표준 Python 데이터 분석 라이브러리 직접 사용
- **결과의 정확성**: JavaScript로 재구현하지 않아 Python 표준 라이브러리와 동일한 결과 보장

### 4.2 모듈 기반 파이프라인 구성

사용자는 시각적 인터페이스를 통해 모듈을 드래그 앤 드롭하여 파이프라인을 구성할 수 있다:

- **시각적 구성**: Canvas 상에서 모듈을 배치하고 연결선으로 데이터 흐름 표현
- **의존성 자동 해결**: 시스템이 모듈 간 의존성을 분석하여 올바른 실행 순서 결정
- **중간 결과 확인**: 각 모듈의 출력 결과를 "View Details"를 통해 확인 가능
- **파이프라인 재사용**: 구성한 파이프라인을 파일로 저장하고 불러올 수 있음

### 4.3 통계 분석 모듈 (Stat Lab)

본 플랫폼은 다양한 통계 분석 기능을 모듈로 제공한다:

#### 4.3.1 이상치 탐지 (Outlier Detector)

이상치 탐지 모듈은 최대 5개의 숫자형 열에 대해 동시에 이상치를 탐지할 수 있다. 제공되는 방법은 다음과 같다:

- **IQR 기반 탐지**: 사분위수 범위(Interquartile Range)를 이용한 이상치 탐지
  - Q1 - 1.5×IQR 미만 또는 Q3 + 1.5×IQR 초과인 값을 이상치로 판단
- **Z-score 기반 탐지**: 표준화 점수를 이용한 이상치 탐지
  - Z-score의 절댓값이 3 이상인 값을 이상치로 판단
- **Isolation Forest**: 머신러닝 기반 고급 이상치 탐지
  - scikit-learn의 IsolationForest 알고리즘 사용
- **Boxplot 기반 탐지**: 박스플롯을 이용한 시각적 이상치 탐지

각 열에 대해 여러 방법을 동시에 적용할 수 있으며, View Details에서 각 방법별 결과를 탭으로 확인할 수 있다. 사용자는 탐지된 이상치를 선택적으로 제거할 수 있으며, 제거된 데이터는 다음 모듈로 전달된다.

#### 4.3.2 가설 검정 (Hypothesis Testing)

가설 검정 모듈은 다양한 통계 검정을 지원하며, 각 검정 방법에 따라 적절한 데이터 타입의 열만 선택할 수 있도록 동적으로 필터링한다:

- **t-검정**:
  - 단일 표본 t-검정: 표본 평균과 모평균 비교
  - 독립 표본 t-검정: 두 독립 그룹 간 평균 비교
  - 대응 표본 t-검정: 동일 집단의 전후 비교
  - 숫자형 열만 선택 가능
- **카이제곱 검정**: 범주형 데이터의 독립성 검정
  - 범주형 열만 선택 가능
- **ANOVA**: 집단 간 평균 비교 (3개 이상 그룹)
  - 숫자형 종속변수와 범주형 독립변수 필요
- **KS-검정**: 분포 비교 (Kolmogorov-Smirnov test)
  - 숫자형 열만 선택 가능
- **Shapiro-Wilk 검정**: 정규성 검정
  - 숫자형 열만 선택 가능
- **Levene 검정**: 등분산성 검정
  - 숫자형 종속변수와 범주형 독립변수 필요

여러 검정을 동시에 선택하여 한 번에 실행할 수 있으며, 각 검정의 결과는 검정 통계량, p-value, 결론, 해석을 포함하여 표시된다.

#### 4.3.3 상관분석 (Correlation)

상관분석 모듈은 다양한 상관계수를 계산하고 시각화한다:

- **Pearson 상관계수**: 선형 상관관계 측정
  - 숫자형 변수 간 선형 관계의 강도와 방향 측정
- **Spearman 상관계수**: 순위 기반 상관관계
  - 비모수적 방법으로 순위 기반 상관관계 측정
- **Kendall 상관계수**: 비모수적 상관계수
  - 순위 기반 상관관계의 또 다른 측정 방법
- **Cramér's V**: 범주형 변수 간 연관성 측정
  - 범주형 변수 간의 연관성 강도 측정 (0~1 범위)

**자동 시각화 기능:**

- **Heatmap 자동 생성**: 상관계수 행렬을 시각화한 히트맵
  - Matplotlib을 사용하여 생성되며, base64 인코딩된 이미지로 반환
- **Pairplot**: 변수 간 관계를 다양한 플롯으로 표현
  - 산점도, 히스토그램, 커널 밀도 추정 등을 포함한 종합 시각화

View Details에서는 각 상관계수 방법별로 탭을 제공하며, Heatmap과 Pairplot은 별도 탭에서 확인할 수 있다.

### 4.4 머신러닝 파이프라인

본 플랫폼은 완전한 머신러닝 워크플로우를 지원한다:

1. **데이터 전처리**: 결측치 처리, 인코딩, 정규화, 리샘플링
2. **데이터 분할**: 학습/테스트 데이터 분할 (계층화 옵션 포함)
3. **모델 정의**: 다양한 머신러닝 알고리즘 선택
4. **모델 학습**: 하이퍼파라미터 튜닝 옵션 포함
5. **예측**: 학습된 모델을 이용한 예측
6. **평가**: 다양한 평가 지표를 통한 성능 측정

각 단계는 독립적인 모듈로 구현되어 있어, 사용자는 필요한 모듈만 선택하여 파이프라인을 구성할 수 있다.

### 4.5 코드 생성 및 검증

시스템은 구성된 파이프라인을 Python 코드로 자동 변환하는 기능을 제공한다:

- **전체 파이프라인 코드 생성**: 모듈 간 연결 관계를 분석하여 실행 가능한 Python 코드 생성
- **변수명 자동 매핑**: 모듈 간 데이터 전달을 위한 변수명 자동 할당
- **실행 순서 보장**: 위상 정렬을 통한 올바른 실행 순서 결정

생성된 코드는 독립적으로 실행 가능하며, 실제 Python 환경에서 동일한 결과를 얻을 수 있다. 이를 통해 결과의 재현성과 검증 가능성을 보장한다.

### 4.6 카테고리별 모듈 상세 설명

#### 4.6.1 Data Preprocess (데이터 전처리)

데이터 전처리 카테고리는 원시 데이터를 분석 가능한 형태로 변환하는 모듈들을 포함한다:

**Load Data**

- **기능**: CSV 파일을 읽어서 pandas DataFrame으로 변환
- **입력**: 없음 (파일 경로 직접 지정)
- **출력**: `DataPreview` (테이블 형태의 데이터)
- **주요 파라미터**: `source` (CSV 파일 경로)
- **사용 예시**: 모든 파이프라인의 시작점으로 사용

**Statistics**

- **기능**: 기술 통계량 계산 (평균, 표준편차, 최소값, 최대값, 분위수 등)
- **입력**: `DataPreview`
- **출력**: `StatisticsOutput`
- **주요 파라미터**: 없음 (모든 숫자형 열에 대해 자동 계산)
- **사용 예시**: 데이터 탐색 및 품질 확인

**Select Data**

- **기능**: 특정 열 선택 또는 제거
- **입력**: `DataPreview`
- **출력**: `DataPreview` (선택된 열만 포함)
- **주요 파라미터**: `columnSelections` (열 선택 맵)
- **사용 예시**: 불필요한 열 제거, 특성 선택

**Handle Missing Values**

- **기능**: 결측치 처리
- **입력**: `DataPreview`
- **출력**: `HandlerOutput` (전처리 핸들러 객체)
- **주요 파라미터**:
  - `method`: "remove_row" (행 제거), "impute" (대체), "knn" (KNN 대체)
  - `strategy`: "mean", "median", "mode" (impute 방법)
  - `n_neighbors`: KNN의 이웃 수
- **사용 예시**: 결측치가 있는 데이터 전처리

**Transform Data**

- **기능**: Handle Missing Values로 생성된 핸들러를 새로운 데이터에 적용
- **입력**: `HandlerOutput`, `DataPreview`
- **출력**: `DataPreview` (전처리된 데이터)
- **주요 파라미터**: `primary_exclude_column`, `exclude_columns`
- **사용 예시**: 학습 데이터와 동일한 전처리를 테스트 데이터에 적용

**Encode Categorical**

- **기능**: 범주형 변수를 숫자로 인코딩 (Label Encoding, One-Hot Encoding)
- **입력**: `DataPreview`
- **출력**: `DataPreview` (인코딩된 데이터)
- **주요 파라미터**: `method` (인코딩 방법), `columns` (인코딩할 열)
- **사용 예시**: 머신러닝 모델 입력을 위한 범주형 변수 변환

**Normalize Data**

- **기능**: 숫자형 데이터 정규화 및 스케일링
- **입력**: `DataPreview`
- **출력**: `DataPreview` (정규화된 데이터)
- **주요 파라미터**: `method` (MinMax, Standard, Robust 등), `columns` (정규화할 열)
- **사용 예시**: 서로 다른 스케일의 특성을 동일한 범위로 변환

**Transition Data**

- **기능**: 수학적 변환 적용 (로그, 제곱근, 제곱 등)
- **입력**: `DataPreview`
- **출력**: `DataPreview` (변환된 데이터)
- **주요 파라미터**: `transformation` (변환 함수), `columns` (변환할 열)
- **사용 예시**: 데이터 분포 정규화, 비선형 관계 선형화

**Resample Data**

- **기능**: 클래스 불균형 데이터 리샘플링
- **입력**: `DataPreview`
- **출력**: `DataPreview` (리샘플링된 데이터)
- **주요 파라미터**: `method` (SMOTE, NearMiss 등), `target_column` (목표 변수)
- **사용 예시**: 불균형 분류 문제 해결

#### 4.6.2 Stat Lab (통계 분석)

Stat Lab 카테고리는 다양한 통계 분석 기능을 제공한다:

**Statistics** (4.3.1에서 상세 설명)

- 기술 통계량 계산
- 평균, 표준편차, 분위수, 최소값, 최대값 등

**Outlier Detector** (4.3.1에서 상세 설명)

- IQR, Z-score, Isolation Forest, Boxplot 기반 이상치 탐지
- 최대 5개 열 동시 분석
- 인터랙티브 이상치 제거

**Hypothesis Testing** (4.3.2에서 상세 설명)

- t-검정, 카이제곱 검정, ANOVA, KS-검정, Shapiro-Wilk, Levene 검정
- 다중 검정 동시 실행
- 동적 열 타입 필터링

**Correlation** (4.3.3에서 상세 설명)

- Pearson, Spearman, Kendall, Cramér's V 상관계수
- Heatmap 및 Pairplot 자동 생성

**Column Plot**

- **기능**: 단일 열에 대한 다양한 시각화
- **입력**: `DataPreview`
- **출력**: `DistributionOutput` (그래프 이미지)
- **주요 파라미터**: `column` (시각화할 열), `plot_type` (히스토그램, 박스플롯 등)
- **사용 예시**: 데이터 분포 확인, 이상치 시각적 탐지

#### 4.6.3 Data Analysis (데이터 분석)

데이터 분석 카테고리는 머신러닝 워크플로우의 핵심 단계를 포함한다:

**Split Data**

- **기능**: 데이터를 학습/테스트 세트로 분할
- **입력**: `DataPreview`
- **출력**: `DataPreview` (학습 데이터), `DataPreview` (테스트 데이터)
- **주요 파라미터**:
  - `test_size`: 테스트 데이터 비율 (0.0~1.0)
  - `random_state`: 재현성을 위한 시드
  - `stratify`: 계층화 샘플링 여부
- **사용 예시**: 모델 학습 전 데이터 분할

**Train Model**

- **기능**: 머신러닝 모델 학습
- **입력**: `TrainedModelOutput` (모델 정의), `DataPreview` (학습 데이터)
- **출력**: `TrainedModelOutput` (학습된 모델)
- **주요 파라미터**:
  - `feature_columns`: 입력 특성 열
  - `label_column`: 목표 변수 열
- **사용 예시**: 모델 학습 수행

**Score Model**

- **기능**: 학습된 모델을 사용하여 예측 수행
- **입력**: `TrainedModelOutput` (학습된 모델), `DataPreview` (예측할 데이터)
- **출력**: `DataPreview` (예측 결과 포함)
- **주요 파라미터**: 없음 (자동으로 예측 열 추가)
- **사용 예시**: 테스트 데이터에 대한 예측 생성

**Evaluate Model**

- **기능**: 모델 성능 평가
- **입력**: `DataPreview` (실제 값과 예측 값 포함)
- **출력**: `EvaluationOutput` (평가 지표)
- **주요 파라미터**:
  - `label_column`: 실제 값 열
  - `prediction_column`: 예측 값 열
  - `model_type`: "regression" 또는 "classification"
  - `threshold`: 분류 임계값 (기본 0.5)
- **사용 예시**: 모델 성능 측정 및 비교

#### 4.6.4 Supervised Learning (지도 학습)

지도 학습 카테고리는 다양한 머신러닝 알고리즘을 제공한다:

**Linear Regression**

- **용도**: 연속형 목표 변수 예측
- **알고리즘**: 최소제곱법 기반 선형 회귀
- **하이퍼파라미터**: `fit_intercept` (절편 포함 여부)
- **사용 예시**: 가격 예측, 수요 예측

**Logistic Regression**

- **용도**: 이진 분류 문제
- **알고리즘**: 로지스틱 함수를 사용한 확률적 분류
- **하이퍼파라미터**: `C` (정규화 강도), `solver` (최적화 알고리즘)
- **사용 예시**: 이탈 예측, 질병 진단

**Poisson Regression**

- **용도**: 카운트 데이터 모델링
- **알고리즘**: 포아송 분포 가정
- **하이퍼파라미터**: 없음
- **사용 예시**: 사고 건수 예측, 이벤트 발생 횟수 예측

**Negative Binomial Regression**

- **용도**: 과분산(overdispersion)이 있는 카운트 데이터
- **알고리즘**: 음이항 분포 가정
- **하이퍼파라미터**: 없음
- **사용 예시**: 과분산 카운트 데이터 모델링

**Decision Tree**

- **용도**: 분류 및 회귀
- **알고리즘**: 의사결정나무 (CART)
- **하이퍼파라미터**: `max_depth`, `min_samples_split`, `criterion`
- **사용 예시**: 해석 가능한 모델이 필요한 경우

**Random Forest**

- **용도**: 분류 및 회귀 (앙상블 방법)
- **알고리즘**: 다수의 의사결정나무 앙상블
- **하이퍼파라미터**: `n_estimators`, `max_depth`, `min_samples_split`
- **사용 예시**: 높은 성능이 필요한 경우

**SVM (Support Vector Machine)**

- **용도**: 분류 및 회귀
- **알고리즘**: 서포트 벡터 머신
- **하이퍼파라미터**: `C`, `kernel` (linear, rbf, poly 등), `gamma`
- **사용 예시**: 비선형 경계가 있는 분류 문제

**LDA (Linear Discriminant Analysis)**

- **용도**: 분류 및 차원 축소
- **알고리즘**: 선형 판별 분석
- **하이퍼파라미터**: `solver`, `shrinkage`
- **사용 예시**: 차원 축소가 필요한 분류 문제

**Naive Bayes**

- **용도**: 분류 (확률적 분류기)
- **알고리즘**: 베이즈 정리 기반
- **하이퍼파라미터**: 없음
- **사용 예시**: 텍스트 분류, 스팸 필터링

**KNN (K-Nearest Neighbors)**

- **용도**: 분류 및 회귀
- **알고리즘**: K-최근접 이웃
- **하이퍼파라미터**: `n_neighbors`, `weights`, `metric`
- **사용 예시**: 지역적 패턴이 중요한 경우

#### 4.6.5 Unsupervised Learning (비지도 학습)

비지도 학습 카테고리는 레이블 없이 데이터의 패턴을 찾는 알고리즘을 제공한다:

**K-Means**

- **용도**: 클러스터링
- **알고리즘**: K-평균 클러스터링
- **하이퍼파라미터**: `n_clusters` (클러스터 수)
- **사용 예시**: 고객 세분화, 시장 세분화

**Hierarchical Clustering**

- **용도**: 계층적 클러스터링
- **알고리즘**: 계층적 클러스터링 (Ward, Complete, Average linkage)
- **하이퍼파라미터**: `n_clusters`, `affinity`, `linkage`
- **사용 예시**: 덴드로그램이 필요한 경우

**DBSCAN**

- **용도**: 밀도 기반 클러스터링
- **알고리즘**: 밀도 기반 공간 클러스터링
- **하이퍼파라미터**: `eps` (이웃 반경), `min_samples` (최소 샘플 수)
- **사용 예시**: 불규칙한 형태의 클러스터 탐지

**PCA (Principal Component Analysis)**

- **용도**: 차원 축소
- **알고리즘**: 주성분 분석
- **하이퍼파라미터**: `n_components` (주성분 수)
- **사용 예시**: 차원 축소, 시각화, 노이즈 제거

#### 4.6.6 Tradition Analysis (전통적 통계 분석)

전통적 통계 분석 카테고리는 통계 모델링 기능을 제공한다:

**Stat Models**

- **기능**: 통계 모델 정의
- **입력**: 없음
- **출력**: `TrainedModelOutput` (모델 정의)
- **주요 파라미터**: `model` (OLS, Logit, Probit 등)
- **사용 예시**: 통계적 추론이 필요한 경우

**Result Model**

- **기능**: 통계 모델 적합 및 요약
- **입력**: `TrainedModelOutput` (모델 정의), `DataPreview` (데이터)
- **출력**: `TrainedModelOutput` (적합된 모델), 통계 요약
- **주요 파라미터**: `formula` (모델 공식)
- **사용 예시**: 회귀 분석, 로지스틱 회귀 분석

**Predict Model**

- **기능**: 적합된 통계 모델로 예측
- **입력**: `TrainedModelOutput` (적합된 모델), `DataPreview` (예측할 데이터)
- **출력**: `DataPreview` (예측 결과)
- **주요 파라미터**: 없음
- **사용 예시**: 통계 모델 기반 예측

---

## 5. Use Cases (사용 사례)

### 5.0 실행 방법 (Execution Guide)

#### 5.0.1 데이터 로드 및 초기 설정

**1단계: 데이터 파일 준비**

- CSV 형식의 데이터 파일을 준비한다
- 파일은 로컬 컴퓨터에 저장되어 있어야 한다
- 첫 번째 행은 열 이름(header)을 포함해야 한다

**2단계: Load Data 모듈 추가**

- 좌측 툴박스에서 "Data Preprocess" 카테고리를 클릭
- "Load Data" 모듈을 Canvas로 드래그 앤 드롭
- 모듈을 클릭하여 Properties Panel 열기

**3단계: 데이터 파일 선택**

- Properties Panel의 "Source" 필드에 CSV 파일 경로 입력
- 또는 파일 선택 버튼을 클릭하여 파일 선택
- 파일이 로드되면 모듈 상태가 "Success"로 변경됨

**4단계: 데이터 확인**

- Load Data 모듈의 "View Details" 버튼 클릭
- 데이터 미리보기에서 행 수, 열 수, 데이터 타입 확인
- 각 열의 샘플 데이터 확인

#### 5.0.2 파이프라인 구성 및 모듈 연결

**1단계: 모듈 추가**

- 필요한 모듈을 툴박스에서 Canvas로 드래그 앤 드롭
- 예: Statistics, Select Data, Handle Missing Values 등

**2단계: 모듈 연결**

- 모듈의 출력 포트(output port)에서 다음 모듈의 입력 포트(input port)로 연결선 그리기
- 연결선은 마우스로 드래그하여 생성
- 데이터 타입이 일치하는 포트만 연결 가능 (data → data, model → model 등)

**3단계: 모듈 파라미터 설정**

- 각 모듈을 클릭하여 Properties Panel에서 파라미터 설정
- 예: Select Data 모듈에서 선택할 열 지정
- 예: Handle Missing Values 모듈에서 결측치 처리 방법 선택

**4단계: 파이프라인 검증**

- 모든 모듈이 올바르게 연결되었는지 확인
- 입력 포트가 연결되지 않은 모듈은 실행되지 않음
- 순환 참조(circular dependency)가 없는지 확인

#### 5.0.3 파이프라인 실행

**1단계: 실행 시작**

- 상단 툴바의 "Run" 버튼 클릭
- 또는 각 모듈의 개별 실행 버튼 클릭

**2단계: 실행 상태 모니터링**

- 각 모듈의 상태가 실시간으로 업데이트됨:
  - **Pending**: 실행 대기 중
  - **Running**: 실행 중 (회전 아이콘 표시)
  - **Success**: 실행 성공 (녹색 체크 표시)
  - **Error**: 실행 실패 (빨간색 X 표시)

**3단계: 결과 확인**

- 성공한 모듈의 "View Details" 버튼 클릭
- 각 모듈의 출력 결과를 상세히 확인
- 결과는 모듈 타입에 따라 테이블, 통계, 그래프 등으로 표시

**4단계: 에러 처리**

- 에러가 발생한 모듈의 상태를 확인
- Properties Panel에서 에러 메시지 확인
- 파라미터를 수정하고 다시 실행

#### 5.0.4 머신러닝 모델 구축 방법

**1단계: 데이터 전처리**

```
Load Data → Select Data → Handle Missing Values →
Encode Categorical → Normalize Data
```

- **Select Data**: 필요한 특성(feature) 열과 목표 변수(label) 열 선택
- **Handle Missing Values**: 결측치 처리 (제거 또는 대체)
- **Encode Categorical**: 범주형 변수를 숫자로 변환
- **Normalize Data**: 데이터 스케일링 (선택사항)

**2단계: 데이터 분할**

```
Normalize Data → Split Data
```

- **Split Data** 모듈 설정:
  - `test_size`: 테스트 데이터 비율 (예: 0.2 = 20%)
  - `random_state`: 재현성을 위한 시드 값
  - `stratify`: 계층화 샘플링 옵션 (분류 문제의 경우)

**3단계: 모델 정의**

- Supervised Learning 카테고리에서 적절한 모델 선택:
  - **회귀 문제**: Linear Regression, Random Forest 등
  - **분류 문제**: Logistic Regression, Decision Tree, SVM 등
- 모델 모듈을 Canvas에 추가

**4단계: 모델 학습**

```
Split Data (train) → [Model] → Train Model
```

- **Train Model** 모듈 설정:
  - `feature_columns`: 입력 특성 열 선택
  - `label_column`: 목표 변수 열 선택
- Train Model 모듈의 입력에 모델 정의와 학습 데이터 연결

**5단계: 예측 수행**

```
Split Data (test) → Train Model (trained_model) → Score Model
```

- **Score Model** 모듈 설정:
  - 테스트 데이터와 학습된 모델을 입력으로 연결
  - 예측 결과가 데이터에 새 열로 추가됨

**6단계: 모델 평가**

```
Score Model → Evaluate Model
```

- **Evaluate Model** 모듈 설정:
  - `label_column`: 실제 값 열
  - `prediction_column`: 예측 값 열
  - `model_type`: "regression" 또는 "classification"
- 평가 지표 자동 계산:
  - **회귀**: RMSE, MAE, R² 등
  - **분류**: Accuracy, Precision, Recall, F1-score 등

**7단계: 결과 해석**

- Evaluate Model의 "View Details"에서 평가 결과 확인
- 혼동 행렬(Confusion Matrix) 확인 (분류 문제)
- 필요시 모델 파라미터 조정 및 재학습

### 5.1 기본 워크플로우

일반적인 데이터 분석 워크플로우는 다음과 같다:

1. **데이터 로드**: Load Data 모듈을 사용하여 CSV 파일 로드
2. **데이터 탐색**: Statistics 모듈로 기술 통계 확인
3. **데이터 전처리**: 필요한 전처리 모듈들을 연결하여 구성
4. **통계 분석**: Stat Lab 카테고리의 모듈들을 활용하여 다양한 통계 분석 수행
5. **모델링**: 머신러닝 모델을 정의하고 학습
6. **평가**: 모델 성능 평가 및 결과 확인

### 5.2 모듈 조합 예시

#### 예시 1: 이상치 탐지 및 제거 파이프라인

```
Load Data → Statistics → Outlier Detector → (이상치 제거) → Statistics
```

이 파이프라인은 데이터를 로드한 후 기술 통계를 확인하고, 이상치를 탐지한 다음 이상치를 제거한 후 다시 통계를 확인하여 변화를 관찰한다.

#### 예시 2: 가설 검정 파이프라인

```
Load Data → Hypothesis Testing (다중 검정 동시 실행)
```

이 파이프라인은 데이터를 로드한 후 여러 가설 검정을 동시에 실행하여 데이터의 특성을 종합적으로 분석한다.

#### 예시 3: 상관분석 및 시각화

```
Load Data → Correlation → (Heatmap, Pairplot 자동 생성)
```

이 파이프라인은 데이터를 로드한 후 상관분석을 수행하고, 결과를 Heatmap과 Pairplot으로 시각화한다.

#### 예시 4: 머신러닝 파이프라인

```
Load Data → Select Data → Handle Missing Values →
Encode Categorical → Normalize Data → Split Data →
Linear Regression → Train Model → Score Model → Evaluate Model
```

이 파이프라인은 완전한 머신러닝 워크플로우를 구현한다. 데이터를 전처리한 후 선형 회귀 모델을 학습하고, 예측을 수행한 다음 모델 성능을 평가한다.

### 5.3 고급 기능 활용

- **다중 검정 동시 실행**: Hypothesis Testing 모듈에서 여러 검정을 한 번에 선택하여 실행 가능
- **인터랙티브 결과 조작**: Outlier Detector의 View Details에서 탐지된 이상치를 선택적으로 제거
- **파이프라인 저장/불러오기**: 구성한 파이프라인을 파일로 저장하고 재사용
- **코드 내보내기**: 파이프라인을 Python 코드로 변환하여 독립 실행 가능

---

## 6. Discussion (토론)

### 6.1 주요 장점

#### 6.1.1 접근성

- **비전문가도 사용 가능**: 프로그래밍 지식 없이도 시각적 인터페이스를 통해 데이터 분석 수행
- **직관적인 UI**: 드래그 앤 드롭 방식의 모듈 구성으로 학습 곡선 최소화
- **즉시 결과 확인**: 각 모듈의 결과를 실시간으로 확인 가능하여 반복적인 분석 과정 지원

#### 6.1.2 검증 가능성

- **Python 코드 기반**: 모든 분석이 Python으로 실행되어 결과의 신뢰성 보장
- **표준 라이브러리 사용**: pandas, scikit-learn 등 검증된 라이브러리 활용
- **코드 생성 기능**: 파이프라인을 Python 코드로 변환하여 독립 실행 및 검증 가능

#### 6.1.3 확장성

- **모듈 기반 구조**: 새로운 분석 기능을 모듈로 추가하여 시스템 확장
- **표준화된 인터페이스**: 모든 모듈이 동일한 입력/출력 인터페이스를 따름
- **재사용 가능성**: 모듈을 조합하여 다양한 분석 파이프라인 구성

#### 6.1.4 효율성

- **다중 분석 동시 실행**: 여러 통계 검정을 한 번에 수행하여 시간 절약
- **중간 결과 확인**: 파이프라인 중간 단계의 결과를 확인하여 문제 조기 발견
- **최적 조합 탐색**: 다양한 모듈 조합을 시도하여 최적의 분석 방법 파악

### 6.2 제한사항 및 향후 개선 방향

#### 6.2.1 성능 제약

- **브라우저 메모리 제한**: 대용량 데이터셋 처리 시 브라우저 메모리 제한에 부딪힐 수 있음
- **초기 로딩 시간**: Pyodide 및 패키지 로딩에 30-90초 소요
- **모바일 브라우저**: 모바일 환경에서의 성능이 데스크톱보다 낮음

**개선 방향**:

- 데이터 샘플링 옵션 제공
- Progressive loading을 통한 초기 로딩 시간 단축
- Web Worker를 활용한 백그라운드 실행

#### 6.2.2 기능 확장

현재 제공되는 통계 분석 모듈 외에도 다음과 같은 기능을 추가할 수 있다:

- **시계열 분석**: ARIMA, 시계열 분해 등
- **생존 분석**: Kaplan-Meier 추정, Cox 회귀 등
- **베이지안 분석**: 베이지안 추론 및 MCMC 샘플링
- **다변량 분석**: 주성분 분석 확장, 요인 분석 등

#### 6.2.3 사용자 경험 개선

- **자동 파이프라인 추천**: 데이터 특성에 따라 적절한 파이프라인 자동 추천
- **결과 해석 가이드**: 통계 결과에 대한 자동 해석 및 가이드 제공
- **협업 기능**: 여러 사용자가 동시에 파이프라인을 편집하고 공유

---

## 7. Conclusion (결론)

본 논문에서는 Pyodide를 활용한 웹 기반 시각적 데이터 분석 플랫폼을 제안하였다. 본 플랫폼의 주요 기여는 다음과 같다:

1. **서버리스 아키텍처**: 별도의 백엔드 서버 없이 브라우저에서 완전한 데이터 분석 워크플로우 제공
2. **Python 표준 라이브러리 활용**: JavaScript로 재구현하지 않고 Python 표준 라이브러리를 직접 사용하여 결과의 정확성 및 검증 가능성 보장
3. **모듈 기반 확장성**: 구조화된 모듈 시스템을 통해 새로운 분석 기능을 쉽게 추가 가능
4. **비전문가 접근성**: 시각적 인터페이스를 통해 프로그래밍 지식이 없는 사용자도 데이터 분석 수행 가능

특히 Stat Lab 카테고리의 모듈들(이상치 탐지, 가설 검정, 상관분석)은 다양한 통계 분석을 한 번에 수행할 수 있도록 설계되어, 연구자와 데이터 분석가들이 효율적으로 데이터를 탐색하고 분석할 수 있도록 지원한다.

향후 추가적인 통계 분석 모듈을 개발하여 더욱 풍부한 분석 기능을 제공할 수 있으며, 모듈 기반 구조 덕분에 이러한 확장이 용이하게 이루어질 수 있다. 또한 성능 최적화 및 사용자 경험 개선을 통해 더욱 실용적인 도구로 발전시킬 수 있을 것이다.

---

## References (참고문헌)

1. Pyodide Documentation. (2024). _Pyodide: Python with the scientific stack, compiled to WebAssembly_. https://pyodide.org/

2. McKinney, W. (2010). Data structures for statistical computing in python. _Proceedings of the 9th Python in Science Conference_, 445, 51-56.

3. Pedregosa, F., et al. (2011). Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12, 2825-2830.

4. Seabold, S., & Perktold, J. (2010). Statsmodels: Econometric and statistical modeling with python. _Proceedings of the 9th Python in Science Conference_, 92-96.

5. Hunter, J. D. (2007). Matplotlib: A 2D graphics environment. _Computing in Science & Engineering_, 9(3), 90-95.

6. Harris, C. R., et al. (2020). Array programming with NumPy. _Nature_, 585(7825), 357-362.

---

**저자 정보** (Author Information)

_본 논문의 저자 정보는 실제 논문 제출 시 추가됩니다._

---

**부록 A: 모듈 목록**

본 플랫폼에서 제공하는 모든 모듈의 목록은 다음과 같다:

- **Data Preprocess**: Load Data, Statistics, Select Data, Transition Data, Resample Data, Handle Missing Values, Encode Categorical, Normalize Data, Transform Data
- **Stat Lab**: Statistics, Outlier Detector, Hypothesis Testing, Correlation, Column Plot
- **Data Analysis**: Split Data, Train Model, Score Model, Evaluate Model
- **Supervised Learning**: Linear Regression, Logistic Regression, Poisson Regression, Negative Binomial Regression, Decision Tree, Random Forest, SVM, LDA, Naive Bayes, KNN
- **Unsupervised Learning**: K-Means, Hierarchical Clustering, DBSCAN, PCA
- **Tradition Analysis**: Stat Models, Result Model, Predict Model

---

**부록 B: 기술 스택**

- **Frontend**: React, TypeScript, Tailwind CSS
- **Python Runtime**: Pyodide v0.24.1 (Python 3.11)
- **Python Libraries**: NumPy 1.24.x, pandas 2.0.x, scikit-learn 1.2.x, SciPy 1.10.x, statsmodels 0.14.x, Matplotlib 3.7.x
- **UI Components**: Heroicons
- **Build Tool**: Vite
- **Package Manager**: pnpm

---

_본 논문은 학술 논문 형식으로 작성되었으며, 실제 논문 제출 시 저널의 형식 가이드라인에 맞춰 수정이 필요할 수 있습니다._
